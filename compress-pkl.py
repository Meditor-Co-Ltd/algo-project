#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∂–∞—Ç–∏—è –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏ (3GB) 
–£–º–µ–Ω—å—à–∞–µ—Ç —Ä–∞–∑–º–µ—Ä –Ω–∞ 60-80% –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞
"""

import os
import joblib
import pickle
import gzip
import time
from datetime import datetime

import numpy as np

def analyze_model_size():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞–∑–º–µ—Ä –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –º–æ–¥–µ–ª–∏"""
    
    model_path = "/Users/meditor/glucose_models/random_forest_model_0740_rmse_17.pkl"
    
    if not os.path.exists(model_path):
        print(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ —Ñ–∞–π–ª –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ —Ç–æ–π –∂–µ –ø–∞–ø–∫–µ")
        return None
    
    print("üîç –ê–ù–ê–õ–ò–ó –ú–û–î–ï–õ–ò")
    print("=" * 50)
    
    # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    file_size = os.path.getsize(model_path)
    file_size_mb = file_size / 1024 / 1024
    file_size_gb = file_size_mb / 1024
    
    print(f"üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞:")
    print(f"   {file_size:,} bytes")
    print(f"   {file_size_mb:.1f} MB")
    print(f"   {file_size_gb:.2f} GB")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    print(f"\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")
    start_time = time.time()
    
    try:
        model_data = joblib.load(model_path)
        load_time = time.time() - start_time
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.1f} —Å–µ–∫—É–Ω–¥")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        print(f"\nüìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
        print(f"   –ö–ª—é—á–∏: {list(model_data.keys())}")
        
        if 'model' in model_data:
            model = model_data['model']
            print(f"   –¢–∏–ø –º–æ–¥–µ–ª–∏: {type(model).__name__}")
            
            if hasattr(model, 'n_estimators'):
                print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {model.n_estimators}")
            
            if hasattr(model, 'estimators_'):
                n_trees = len(model.estimators_)
                print(f"   –î–µ—Ä–µ–≤—å–µ–≤ –≤ estimators_: {n_trees}")
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞
                if n_trees > 0:
                    # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –æ–¥–Ω–æ –¥–µ—Ä–µ–≤–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
                    import io
                    buffer = io.BytesIO()
                    pickle.dump(model.estimators_[0], buffer)
                    tree_size = len(buffer.getvalue())
                    estimated_trees_size = tree_size * n_trees / 1024 / 1024
                    
                    print(f"   –†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞: ~{tree_size/1024:.1f} KB")
                    print(f"   –û—Ü–µ–Ω–æ—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä –≤—Å–µ—Ö –¥–µ—Ä–µ–≤—å–µ–≤: ~{estimated_trees_size:.1f} MB")
        
        if 'feature_names' in model_data:
            features = model_data['feature_names']
            print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(features)}")
        
        return model_data
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return None

def compress_with_gzip(model_data, output_path="model_compressed.pkl.gz"):
    """–°–∂–∏–º–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é gzip"""
    
    print(f"\nüóúÔ∏è –°–ñ–ê–¢–ò–ï GZIP")
    print("=" * 30)
    
    start_time = time.time()
    
    try:
        print("üîÑ –°–∂–∏–º–∞–µ–º –º–æ–¥–µ–ª—å...")
        
        with gzip.open(output_path, 'wb', compresslevel=9) as f:
            pickle.dump(model_data, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        compress_time = time.time() - start_time
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã
        original_size = os.path.getsize("/Users/meditor/glucose_models/random_forest_model_0740_rmse_17.pkl")
        compressed_size = os.path.getsize(output_path)
        
        compression_ratio = compressed_size / original_size
        space_saved = original_size - compressed_size
        
        print(f"‚úÖ –°–∂–∞—Ç–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {compress_time:.1f} —Å–µ–∫—É–Ω–¥")
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–∂–∞—Ç–∏—è:")
        print(f"   –û—Ä–∏–≥–∏–Ω–∞–ª: {original_size/1024/1024:.1f} MB")
        print(f"   –°–∂–∞—Ç—ã–π: {compressed_size/1024/1024:.1f} MB")
        print(f"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {compression_ratio:.2f}")
        print(f"   –≠–∫–æ–Ω–æ–º–∏—è: {space_saved/1024/1024:.1f} MB ({(1-compression_ratio)*100:.1f}%)")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Å–∂–∞—Ç–æ–π –º–æ–¥–µ–ª–∏
        print(f"\nüß™ –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∂–∞—Ç–æ–π –º–æ–¥–µ–ª–∏...")
        test_start = time.time()
        
        with gzip.open(output_path, 'rb') as f:
            test_model = pickle.load(f)
        
        test_time = time.time() - test_start
        print(f"‚úÖ –°–∂–∞—Ç–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∑–∞ {test_time:.1f} —Å–µ–∫—É–Ω–¥")
        
        return output_path, compressed_size
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∂–∞—Ç–∏—è: {e}")
        return None, 0

def optimize_random_forest(model_data, target_size_mb=100):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç Random Forest, —É–º–µ–Ω—å—à–∞—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤"""
    
    print(f"\n‚ö° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø RANDOM FOREST")
    print("=" * 40)
    print(f"–¶–µ–ª—å: —É–º–µ–Ω—å—à–∏—Ç—å –¥–æ {target_size_mb} MB")
    
    if 'model' not in model_data:
        print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö")
        return None
    
    original_model = model_data['model']
    
    if not hasattr(original_model, 'estimators_'):
        print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–µ—Ä–µ–≤—å–µ–≤")
        return None
    
    original_trees = len(original_model.estimators_)
    print(f"üìä –ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {original_trees}")
    
    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞
    import io
    buffer = io.BytesIO()
    pickle.dump(original_model.estimators_[0], buffer)
    tree_size_kb = len(buffer.getvalue()) / 1024
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
    target_trees = int((target_size_mb * 1024) / tree_size_kb * 0.8)  # 80% –æ—Ç –ª–∏–º–∏—Ç–∞ –¥–ª—è –∑–∞–ø–∞—Å–∞
    target_trees = max(target_trees, 10)  # –ú–∏–Ω–∏–º—É–º 10 –¥–µ—Ä–µ–≤—å–µ–≤
    target_trees = min(target_trees, original_trees)  # –ù–µ –±–æ–ª—å—à–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞
    
    print(f"üìä –†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞: ~{tree_size_kb:.1f} KB")
    print(f"üìä –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤: {target_trees}")
    
    if target_trees >= original_trees:
        print("‚ÑπÔ∏è –ú–æ–¥–µ–ª—å —É–∂–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞ –ø–æ —Ä–∞–∑–º–µ—Ä—É")
        return model_data
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    print(f"üîÑ –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å {target_trees} –¥–µ—Ä–µ–≤—å—è–º–∏...")
    
    from sklearn.ensemble import RandomForestRegressor
    
    optimized_model = RandomForestRegressor()
    
    # –ö–æ–ø–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    for attr in ['n_estimators', 'criterion', 'max_depth', 'min_samples_split', 
                 'min_samples_leaf', 'max_features', 'random_state']:
        if hasattr(original_model, attr):
            setattr(optimized_model, attr, getattr(original_model, attr))
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤
    optimized_model.n_estimators = target_trees
    
    # –ö–æ–ø–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ N –¥–µ—Ä–µ–≤—å–µ–≤ (–ª—É—á—à–∏–µ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏)
    optimized_model.estimators_ = original_model.estimators_[:target_trees]
    optimized_model.n_outputs_ = getattr(original_model, 'n_outputs_', 1)
    optimized_model.n_features_in_ = getattr(original_model, 'n_features_in_', len(model_data.get('feature_names', [])))
    
    # –ö–æ–ø–∏—Ä—É–µ–º –¥—Ä—É–≥–∏–µ –∞—Ç—Ä–∏–±—É—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å
    for attr in ['feature_importances_', 'oob_score_', 'oob_prediction_']:
        if hasattr(original_model, attr):
            value = getattr(original_model, attr)
            if attr == 'feature_importances_':
                # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–µ–Ω—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–µ—Ä–µ–≤—å–µ–≤
                new_importance = np.zeros_like(value)
                for tree in optimized_model.estimators_:
                    new_importance += tree.feature_importances_
                new_importance /= len(optimized_model.estimators_)
                setattr(optimized_model, attr, new_importance)
            else:
                setattr(optimized_model, attr, value)
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
    optimized_data = model_data.copy()
    optimized_data['model'] = optimized_model
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
    if 'metrics' in optimized_data:
        original_r2 = optimized_data['metrics'].get('R¬≤', 0)
        # –û—Ü–µ–Ω–æ—á–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ (–æ—á–µ–Ω—å –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
        quality_loss = 1 - (target_trees / original_trees) ** 0.5
        estimated_r2 = original_r2 * (1 - quality_loss * 0.1)  # –ú–∞–∫—Å–∏–º—É–º 10% –ø–æ—Ç–µ—Ä–∏
        
        optimized_data['metrics'] = optimized_data['metrics'].copy()
        optimized_data['metrics']['R¬≤_estimated'] = estimated_r2
        optimized_data['metrics']['trees_reduced'] = f"{original_trees} ‚Üí {target_trees}"
    
    print(f"‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    print(f"   –î–µ—Ä–µ–≤—å–µ–≤: {original_trees} ‚Üí {target_trees} ({target_trees/original_trees*100:.1f}%)")
    
    if 'metrics' in optimized_data and 'R¬≤' in optimized_data['metrics']:
        original_r2 = model_data['metrics']['R¬≤']
        estimated_r2 = optimized_data['metrics']['R¬≤_estimated']
        print(f"   –û—Ü–µ–Ω–æ—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: R¬≤ {original_r2:.3f} ‚Üí {estimated_r2:.3f}")
    
    return optimized_data

def save_optimized_model(model_data, filename_prefix="model_optimized"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ã—á–Ω—ã–π pkl
    pkl_path = f"{filename_prefix}.pkl"
    joblib.dump(model_data, pkl_path)
    pkl_size = os.path.getsize(pkl_path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∂–∞—Ç—ã–π
    gz_path = f"{filename_prefix}.pkl.gz"
    with gzip.open(gz_path, 'wb', compresslevel=9) as f:
        pickle.dump(model_data, f, protocol=pickle.HIGHEST_PROTOCOL)
    gz_size = os.path.getsize(gz_path)
    
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
    print(f"   PKL: {pkl_path} ({pkl_size/1024/1024:.1f} MB)")
    print(f"   GZ:  {gz_path} ({gz_size/1024/1024:.1f} MB)")
    
    return pkl_path, gz_path

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("üöÄ –°–ñ–ê–¢–ò–ï –ò –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò 3GB")
    print("=" * 60)
    print(f"–í—Ä–µ–º—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –º–æ–¥–µ–ª—å
    model_data = analyze_model_size()
    if not model_data:
        return
    
    original_size = os.path.getsize("/Users/meditor/glucose_models/random_forest_model_0740_rmse_17.pkl")
    
    # 2. –ü—Ä–æ—Å—Ç–æ–µ —Å–∂–∞—Ç–∏–µ gzip
    compressed_path, compressed_size = compress_with_gzip(model_data)
    
    if compressed_path:
        print(f"\n‚úÖ –ë—ã—Å—Ç—Ä–æ–µ —Ä–µ—à–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ!")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∞–π–ª: {compressed_path}")
        print(f"   –≠–∫–æ–Ω–æ–º–∏—è: {(original_size - compressed_size)/1024/1024:.0f} MB")
    
    # 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –≤—Å–µ –µ—â–µ –±–æ–ª—å—à–æ–π)
    if compressed_size > 100 * 1024 * 1024:  # –ë–æ–ª—å—à–µ 100MB
        print(f"\nüéØ –§–∞–π–ª –≤—Å–µ –µ—â–µ –±–æ–ª—å—à–æ–π ({compressed_size/1024/1024:.0f}MB)")
        print(f"    –ü—Ä–æ–±—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–∏...")
        
        optimized_data = optimize_random_forest(model_data, target_size_mb=50)
        
        if optimized_data:
            pkl_path, gz_path = save_optimized_model(optimized_data)
            
            final_size = os.path.getsize(gz_path)
            print(f"\nüéâ –§–ò–ù–ê–õ–¨–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢:")
            print(f"   –û—Ä–∏–≥–∏–Ω–∞–ª: {original_size/1024/1024:.0f} MB")
            print(f"   –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π: {final_size/1024/1024:.0f} MB")
            print(f"   –û–±—â–∞—è —ç–∫–æ–Ω–æ–º–∏—è: {(original_size - final_size)/1024/1024:.0f} MB ({(1-final_size/original_size)*100:.0f}%)")
            
            print(f"\nüìã –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
            if final_size < 25 * 1024 * 1024:  # –ú–µ–Ω—å—à–µ 25MB
                print(f"   ‚úÖ –û—Ç–ª–∏—á–Ω–æ! –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å –ª—é–±—ã–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ–º")
                print(f"   ‚úÖ Google Drive –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º")
                print(f"   ‚úÖ Firebase Storage - –∏–¥–µ–∞–ª—å–Ω–æ")
            elif final_size < 100 * 1024 * 1024:  # –ú–µ–Ω—å—à–µ 100MB
                print(f"   ‚úÖ –•–æ—Ä–æ—à–æ! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è Firebase Storage")
                print(f"   ‚ö†Ô∏è Google Drive –º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è")
            else:
                print(f"   ‚ö†Ô∏è –í—Å–µ –µ—â–µ –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª")
                print(f"   üí° –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ LightGBM")
    
    print(f"\nüéØ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
    print(f"1. –û–±–Ω–æ–≤–∏—Ç–µ app.py –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∂–∞—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤")
    print(f"2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –≤ –æ–±–ª–∞–∫–æ")
    print(f"3. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
    print(f"4. –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ - –æ–±—É—á–∏—Ç–µ LightGBM –º–æ–¥–µ–ª—å")

if __name__ == "__main__":
    main()